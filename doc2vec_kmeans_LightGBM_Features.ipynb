{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/opt/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "def test( test):\n",
    "    model = Doc2Vec.load(\"model_test\")\n",
    "    \n",
    "    inferred_vector_dm = model.infer_vector(doc_words=test)\n",
    "    sims = model.docvecs.most_similar([inferred_vector_dm], topn=10)\n",
    "    return sims\n",
    "\n",
    "file_name = \"data.csv\"\n",
    "\n",
    "raw_data = pd.read_csv(file_name)\n",
    "df = pd.DataFrame(raw_data)\n",
    "x_train = []\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    document = TaggededDocument(row['Comment'], tags=[i])\n",
    "    x_train.append(document)\n",
    "    \n",
    "model = Doc2Vec(x_train,min_count=1, window = 3, size = 150, sample=1e-3, workers=4,hs=1,iter=6)\n",
    "model.train(x_train, total_examples=model.corpus_count, epochs=30)\n",
    "model.save('model_test')\n",
    "\n",
    "        \n",
    "#    test = ...\n",
    "#    sims = test()\n",
    "#    for count, sim in sims:\n",
    "#        sentence = str(x_train[count])\n",
    "        # sentence = x_train[count]\n",
    "        # print('sentence:'+sentence)\n",
    "        # print('sim:'+str(sim))\n",
    "#        print(sentence, sim, len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, 150)) for z in x_train]  # tags[0]是根据标签找向量，reshape的意思是重新按照这个矩阵大小排列。例如 a = np.arange(6).reshape((3, 2)) 输出([[0, 1],[2, 3],[4, 5]])\n",
    "X = np.concatenate(vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125056, 150)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, init='k-means++', n_init=5).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clist_10 = []\n",
    "for label in kmeans.labels_:\n",
    "    clist_10.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15, init='k-means++', n_init=5).fit(X)\n",
    "\n",
    "clist_15 = []\n",
    "for label in kmeans.labels_:\n",
    "    clist_15.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20, init='k-means++', n_init=5).fit(X)\n",
    "\n",
    "clist_20= []\n",
    "for label in kmeans.labels_:\n",
    "    clist_20.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dic = {'clus_10':clist_10,'clus_15':clist_15,'clus_20':clist_20}\n",
    "clus_df = pd.DataFrame(tmp_dic, columns=['clus_10', 'clus_15', 'clus_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docv_df = pd.DataFrame(X)\n",
    "res = pd.concat([clus_df, docv_df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clus_10</th>\n",
       "      <th>clus_15</th>\n",
       "      <th>clus_20</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0.454749</td>\n",
       "      <td>0.075933</td>\n",
       "      <td>0.989870</td>\n",
       "      <td>0.763248</td>\n",
       "      <td>0.790644</td>\n",
       "      <td>-0.251485</td>\n",
       "      <td>-0.316166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451340</td>\n",
       "      <td>-0.205073</td>\n",
       "      <td>0.959361</td>\n",
       "      <td>0.243185</td>\n",
       "      <td>-0.960646</td>\n",
       "      <td>-1.106796</td>\n",
       "      <td>-1.120893</td>\n",
       "      <td>-0.754034</td>\n",
       "      <td>0.301842</td>\n",
       "      <td>-0.711453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.211989</td>\n",
       "      <td>0.485131</td>\n",
       "      <td>-0.756601</td>\n",
       "      <td>0.339424</td>\n",
       "      <td>-0.324144</td>\n",
       "      <td>-2.769078</td>\n",
       "      <td>-1.416410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>-0.467021</td>\n",
       "      <td>-0.659951</td>\n",
       "      <td>0.601387</td>\n",
       "      <td>0.631507</td>\n",
       "      <td>0.560771</td>\n",
       "      <td>-0.648289</td>\n",
       "      <td>-1.247034</td>\n",
       "      <td>0.201610</td>\n",
       "      <td>-0.238821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.517644</td>\n",
       "      <td>-0.483260</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>1.264967</td>\n",
       "      <td>-0.322865</td>\n",
       "      <td>-1.913158</td>\n",
       "      <td>0.256509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583523</td>\n",
       "      <td>-1.085528</td>\n",
       "      <td>1.698945</td>\n",
       "      <td>-0.438385</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>0.990588</td>\n",
       "      <td>-0.360752</td>\n",
       "      <td>0.734434</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>-0.641732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.731719</td>\n",
       "      <td>-0.488490</td>\n",
       "      <td>-0.591593</td>\n",
       "      <td>1.106247</td>\n",
       "      <td>-1.671839</td>\n",
       "      <td>0.311964</td>\n",
       "      <td>-0.216956</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159797</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>-2.014061</td>\n",
       "      <td>-0.528511</td>\n",
       "      <td>-1.246807</td>\n",
       "      <td>1.182403</td>\n",
       "      <td>-0.085697</td>\n",
       "      <td>1.188419</td>\n",
       "      <td>-2.269073</td>\n",
       "      <td>0.627365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.160218</td>\n",
       "      <td>-0.414370</td>\n",
       "      <td>1.155223</td>\n",
       "      <td>-0.110876</td>\n",
       "      <td>-0.330072</td>\n",
       "      <td>0.289665</td>\n",
       "      <td>0.792875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194380</td>\n",
       "      <td>-0.244511</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>-0.049117</td>\n",
       "      <td>0.685791</td>\n",
       "      <td>-0.149767</td>\n",
       "      <td>0.060965</td>\n",
       "      <td>-0.474822</td>\n",
       "      <td>-0.149615</td>\n",
       "      <td>-0.258662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.104035</td>\n",
       "      <td>0.856233</td>\n",
       "      <td>1.013289</td>\n",
       "      <td>-0.253802</td>\n",
       "      <td>1.496202</td>\n",
       "      <td>1.689694</td>\n",
       "      <td>-0.490203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021130</td>\n",
       "      <td>-0.551046</td>\n",
       "      <td>0.404666</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>-0.248470</td>\n",
       "      <td>-1.250434</td>\n",
       "      <td>0.536455</td>\n",
       "      <td>-0.117993</td>\n",
       "      <td>1.255967</td>\n",
       "      <td>-0.708034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.931717</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.820053</td>\n",
       "      <td>-0.319068</td>\n",
       "      <td>-0.958943</td>\n",
       "      <td>-1.393632</td>\n",
       "      <td>0.993937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.923889</td>\n",
       "      <td>-0.264479</td>\n",
       "      <td>-1.290142</td>\n",
       "      <td>-1.570770</td>\n",
       "      <td>-0.458085</td>\n",
       "      <td>0.702444</td>\n",
       "      <td>0.403883</td>\n",
       "      <td>0.453409</td>\n",
       "      <td>-0.228465</td>\n",
       "      <td>-1.947498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894262</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>1.392853</td>\n",
       "      <td>-0.823165</td>\n",
       "      <td>0.986922</td>\n",
       "      <td>0.650972</td>\n",
       "      <td>1.674797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688301</td>\n",
       "      <td>-0.993085</td>\n",
       "      <td>0.248002</td>\n",
       "      <td>-0.135104</td>\n",
       "      <td>0.337365</td>\n",
       "      <td>-0.442643</td>\n",
       "      <td>-1.246762</td>\n",
       "      <td>0.495055</td>\n",
       "      <td>-1.457670</td>\n",
       "      <td>-0.742216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.484781</td>\n",
       "      <td>-0.156893</td>\n",
       "      <td>-0.570019</td>\n",
       "      <td>-0.098150</td>\n",
       "      <td>-0.078652</td>\n",
       "      <td>-0.264801</td>\n",
       "      <td>1.004178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402525</td>\n",
       "      <td>-1.039898</td>\n",
       "      <td>-0.820029</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>-1.238249</td>\n",
       "      <td>-1.091168</td>\n",
       "      <td>0.555323</td>\n",
       "      <td>-0.711731</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>-0.502651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.074618</td>\n",
       "      <td>0.186892</td>\n",
       "      <td>0.523038</td>\n",
       "      <td>-1.221658</td>\n",
       "      <td>-0.575767</td>\n",
       "      <td>-0.601467</td>\n",
       "      <td>1.831380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937187</td>\n",
       "      <td>-0.742368</td>\n",
       "      <td>-0.776615</td>\n",
       "      <td>-0.923591</td>\n",
       "      <td>1.090211</td>\n",
       "      <td>-0.154619</td>\n",
       "      <td>-1.139419</td>\n",
       "      <td>-1.251905</td>\n",
       "      <td>-1.746036</td>\n",
       "      <td>0.599412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.319077</td>\n",
       "      <td>0.189111</td>\n",
       "      <td>0.735804</td>\n",
       "      <td>0.775532</td>\n",
       "      <td>0.546122</td>\n",
       "      <td>-1.863112</td>\n",
       "      <td>-2.048345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018419</td>\n",
       "      <td>-1.438737</td>\n",
       "      <td>0.933700</td>\n",
       "      <td>0.731022</td>\n",
       "      <td>-0.826978</td>\n",
       "      <td>-0.855350</td>\n",
       "      <td>-0.390728</td>\n",
       "      <td>1.940610</td>\n",
       "      <td>-0.387916</td>\n",
       "      <td>1.127038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.238495</td>\n",
       "      <td>0.343419</td>\n",
       "      <td>0.288339</td>\n",
       "      <td>-1.686531</td>\n",
       "      <td>0.186190</td>\n",
       "      <td>0.620981</td>\n",
       "      <td>0.112574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997840</td>\n",
       "      <td>2.253281</td>\n",
       "      <td>-0.479863</td>\n",
       "      <td>1.162844</td>\n",
       "      <td>-0.851110</td>\n",
       "      <td>0.431434</td>\n",
       "      <td>-0.747527</td>\n",
       "      <td>-1.794214</td>\n",
       "      <td>0.811555</td>\n",
       "      <td>2.096765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1.532298</td>\n",
       "      <td>-0.068005</td>\n",
       "      <td>-0.431739</td>\n",
       "      <td>0.518090</td>\n",
       "      <td>-0.961575</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>-1.000568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591483</td>\n",
       "      <td>-0.204968</td>\n",
       "      <td>1.518610</td>\n",
       "      <td>0.444101</td>\n",
       "      <td>-0.342793</td>\n",
       "      <td>0.981514</td>\n",
       "      <td>-0.538393</td>\n",
       "      <td>-0.404810</td>\n",
       "      <td>-1.295646</td>\n",
       "      <td>0.065177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>1.266990</td>\n",
       "      <td>-0.453994</td>\n",
       "      <td>0.568036</td>\n",
       "      <td>-0.423422</td>\n",
       "      <td>-0.300479</td>\n",
       "      <td>0.562464</td>\n",
       "      <td>0.980908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479555</td>\n",
       "      <td>0.624813</td>\n",
       "      <td>-0.465263</td>\n",
       "      <td>-0.383440</td>\n",
       "      <td>0.527489</td>\n",
       "      <td>-0.842416</td>\n",
       "      <td>-0.352087</td>\n",
       "      <td>-1.227127</td>\n",
       "      <td>-0.822321</td>\n",
       "      <td>-0.820280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1.789864</td>\n",
       "      <td>-0.113671</td>\n",
       "      <td>-0.638692</td>\n",
       "      <td>-0.583688</td>\n",
       "      <td>-2.120926</td>\n",
       "      <td>-0.727127</td>\n",
       "      <td>-0.526959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805959</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>-1.322826</td>\n",
       "      <td>-0.678000</td>\n",
       "      <td>-0.242413</td>\n",
       "      <td>-0.500054</td>\n",
       "      <td>-0.454481</td>\n",
       "      <td>0.047415</td>\n",
       "      <td>-1.799811</td>\n",
       "      <td>0.402607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.643194</td>\n",
       "      <td>0.559813</td>\n",
       "      <td>0.556135</td>\n",
       "      <td>-0.707759</td>\n",
       "      <td>-0.267755</td>\n",
       "      <td>1.000388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236951</td>\n",
       "      <td>-0.076833</td>\n",
       "      <td>-1.385507</td>\n",
       "      <td>-0.820939</td>\n",
       "      <td>0.123627</td>\n",
       "      <td>0.066654</td>\n",
       "      <td>-0.444164</td>\n",
       "      <td>0.701125</td>\n",
       "      <td>-0.623872</td>\n",
       "      <td>0.497334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1.586111</td>\n",
       "      <td>-0.162036</td>\n",
       "      <td>-0.381543</td>\n",
       "      <td>0.152505</td>\n",
       "      <td>-0.004210</td>\n",
       "      <td>0.948998</td>\n",
       "      <td>-0.384120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441705</td>\n",
       "      <td>-0.092109</td>\n",
       "      <td>-0.439628</td>\n",
       "      <td>-1.525549</td>\n",
       "      <td>-0.210573</td>\n",
       "      <td>1.337417</td>\n",
       "      <td>1.030276</td>\n",
       "      <td>-0.638091</td>\n",
       "      <td>-0.539717</td>\n",
       "      <td>-0.194077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.593465</td>\n",
       "      <td>-0.164775</td>\n",
       "      <td>-1.129564</td>\n",
       "      <td>-1.765515</td>\n",
       "      <td>1.155397</td>\n",
       "      <td>-0.862518</td>\n",
       "      <td>-0.198087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277721</td>\n",
       "      <td>-0.549283</td>\n",
       "      <td>0.752677</td>\n",
       "      <td>-1.643069</td>\n",
       "      <td>-2.113327</td>\n",
       "      <td>-0.315380</td>\n",
       "      <td>0.453948</td>\n",
       "      <td>-1.285405</td>\n",
       "      <td>2.025407</td>\n",
       "      <td>-0.179060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.988596</td>\n",
       "      <td>0.175324</td>\n",
       "      <td>-0.777983</td>\n",
       "      <td>0.420834</td>\n",
       "      <td>-1.294988</td>\n",
       "      <td>-0.210168</td>\n",
       "      <td>0.124326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617670</td>\n",
       "      <td>-0.033027</td>\n",
       "      <td>1.052647</td>\n",
       "      <td>-0.903474</td>\n",
       "      <td>-1.639873</td>\n",
       "      <td>1.109817</td>\n",
       "      <td>-1.363804</td>\n",
       "      <td>-0.396597</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>-0.478790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0.838402</td>\n",
       "      <td>1.848023</td>\n",
       "      <td>1.901385</td>\n",
       "      <td>-0.825645</td>\n",
       "      <td>-0.087709</td>\n",
       "      <td>-0.051559</td>\n",
       "      <td>2.279064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243672</td>\n",
       "      <td>-0.828519</td>\n",
       "      <td>-0.281909</td>\n",
       "      <td>0.950659</td>\n",
       "      <td>0.194299</td>\n",
       "      <td>0.793625</td>\n",
       "      <td>0.323166</td>\n",
       "      <td>0.768904</td>\n",
       "      <td>-0.419927</td>\n",
       "      <td>-0.104330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.718814</td>\n",
       "      <td>0.276568</td>\n",
       "      <td>-0.317922</td>\n",
       "      <td>0.180582</td>\n",
       "      <td>-0.028598</td>\n",
       "      <td>-0.031075</td>\n",
       "      <td>-0.011744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896637</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>-1.071132</td>\n",
       "      <td>0.982362</td>\n",
       "      <td>0.424636</td>\n",
       "      <td>1.162848</td>\n",
       "      <td>-0.668527</td>\n",
       "      <td>-0.451883</td>\n",
       "      <td>-0.462184</td>\n",
       "      <td>-0.084663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.998192</td>\n",
       "      <td>-0.652060</td>\n",
       "      <td>-0.422096</td>\n",
       "      <td>0.006808</td>\n",
       "      <td>-0.890774</td>\n",
       "      <td>-0.302304</td>\n",
       "      <td>-0.356649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.974528</td>\n",
       "      <td>-0.384752</td>\n",
       "      <td>-1.045449</td>\n",
       "      <td>0.307190</td>\n",
       "      <td>1.149969</td>\n",
       "      <td>-0.411039</td>\n",
       "      <td>0.214585</td>\n",
       "      <td>-1.041682</td>\n",
       "      <td>-0.756979</td>\n",
       "      <td>0.309792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.763630</td>\n",
       "      <td>2.071971</td>\n",
       "      <td>0.389227</td>\n",
       "      <td>0.320396</td>\n",
       "      <td>-0.058987</td>\n",
       "      <td>0.661099</td>\n",
       "      <td>-1.855587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901545</td>\n",
       "      <td>-0.019229</td>\n",
       "      <td>0.623758</td>\n",
       "      <td>0.348588</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.376039</td>\n",
       "      <td>0.353263</td>\n",
       "      <td>1.895003</td>\n",
       "      <td>-0.472101</td>\n",
       "      <td>1.287589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358129</td>\n",
       "      <td>-0.345329</td>\n",
       "      <td>0.344043</td>\n",
       "      <td>1.658000</td>\n",
       "      <td>-0.856290</td>\n",
       "      <td>1.158707</td>\n",
       "      <td>-0.548496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071590</td>\n",
       "      <td>-0.822102</td>\n",
       "      <td>-1.099156</td>\n",
       "      <td>0.239352</td>\n",
       "      <td>0.347223</td>\n",
       "      <td>2.392717</td>\n",
       "      <td>0.124059</td>\n",
       "      <td>-0.760795</td>\n",
       "      <td>-0.385483</td>\n",
       "      <td>-0.475890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.854921</td>\n",
       "      <td>-1.086504</td>\n",
       "      <td>-1.961570</td>\n",
       "      <td>-0.992653</td>\n",
       "      <td>-2.385791</td>\n",
       "      <td>-0.412670</td>\n",
       "      <td>-0.312254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094751</td>\n",
       "      <td>-0.835020</td>\n",
       "      <td>-1.108176</td>\n",
       "      <td>-2.781423</td>\n",
       "      <td>-0.597115</td>\n",
       "      <td>1.106567</td>\n",
       "      <td>1.931757</td>\n",
       "      <td>0.022389</td>\n",
       "      <td>-0.160121</td>\n",
       "      <td>0.677379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.564789</td>\n",
       "      <td>-0.459840</td>\n",
       "      <td>0.375916</td>\n",
       "      <td>-1.250074</td>\n",
       "      <td>-1.675026</td>\n",
       "      <td>-0.239494</td>\n",
       "      <td>1.446652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158307</td>\n",
       "      <td>1.896269</td>\n",
       "      <td>-0.576279</td>\n",
       "      <td>-0.034878</td>\n",
       "      <td>0.963296</td>\n",
       "      <td>-1.188844</td>\n",
       "      <td>0.907049</td>\n",
       "      <td>-0.221215</td>\n",
       "      <td>-0.303974</td>\n",
       "      <td>1.178788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1.206237</td>\n",
       "      <td>-1.563462</td>\n",
       "      <td>1.363556</td>\n",
       "      <td>-1.911910</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>1.409319</td>\n",
       "      <td>0.272559</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266391</td>\n",
       "      <td>0.463811</td>\n",
       "      <td>1.795530</td>\n",
       "      <td>0.658047</td>\n",
       "      <td>0.583725</td>\n",
       "      <td>1.352628</td>\n",
       "      <td>-0.738504</td>\n",
       "      <td>-0.855608</td>\n",
       "      <td>0.271130</td>\n",
       "      <td>-0.046765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.301745</td>\n",
       "      <td>-0.926932</td>\n",
       "      <td>0.378548</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>0.011444</td>\n",
       "      <td>-0.235807</td>\n",
       "      <td>-0.235270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620800</td>\n",
       "      <td>-0.082128</td>\n",
       "      <td>0.150910</td>\n",
       "      <td>0.110486</td>\n",
       "      <td>-0.153438</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.050974</td>\n",
       "      <td>-0.023183</td>\n",
       "      <td>-0.233290</td>\n",
       "      <td>0.360279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.030745</td>\n",
       "      <td>-0.505959</td>\n",
       "      <td>0.018151</td>\n",
       "      <td>-0.250200</td>\n",
       "      <td>-0.594774</td>\n",
       "      <td>0.295514</td>\n",
       "      <td>-0.216311</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223554</td>\n",
       "      <td>-0.438636</td>\n",
       "      <td>0.318239</td>\n",
       "      <td>1.879167</td>\n",
       "      <td>-2.518092</td>\n",
       "      <td>1.056992</td>\n",
       "      <td>0.053190</td>\n",
       "      <td>-2.259646</td>\n",
       "      <td>0.312910</td>\n",
       "      <td>0.011311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.085988</td>\n",
       "      <td>0.371518</td>\n",
       "      <td>-0.122883</td>\n",
       "      <td>0.895685</td>\n",
       "      <td>1.162060</td>\n",
       "      <td>-0.294194</td>\n",
       "      <td>0.755688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599642</td>\n",
       "      <td>-0.694994</td>\n",
       "      <td>-1.475784</td>\n",
       "      <td>-0.066147</td>\n",
       "      <td>-0.289666</td>\n",
       "      <td>-0.500087</td>\n",
       "      <td>-0.862967</td>\n",
       "      <td>-0.728902</td>\n",
       "      <td>0.789504</td>\n",
       "      <td>0.308620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125026</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.775374</td>\n",
       "      <td>-0.861869</td>\n",
       "      <td>0.719364</td>\n",
       "      <td>-0.749225</td>\n",
       "      <td>-0.924174</td>\n",
       "      <td>-0.019475</td>\n",
       "      <td>0.698560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214010</td>\n",
       "      <td>-0.141264</td>\n",
       "      <td>0.706353</td>\n",
       "      <td>-0.168059</td>\n",
       "      <td>-0.131127</td>\n",
       "      <td>0.141395</td>\n",
       "      <td>0.089671</td>\n",
       "      <td>-0.784405</td>\n",
       "      <td>-0.604153</td>\n",
       "      <td>-0.610378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125027</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.678328</td>\n",
       "      <td>-0.676052</td>\n",
       "      <td>0.161326</td>\n",
       "      <td>-0.193364</td>\n",
       "      <td>-0.160909</td>\n",
       "      <td>-0.070157</td>\n",
       "      <td>0.120412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272701</td>\n",
       "      <td>0.232765</td>\n",
       "      <td>-0.365083</td>\n",
       "      <td>0.328134</td>\n",
       "      <td>-0.281861</td>\n",
       "      <td>0.316981</td>\n",
       "      <td>-0.205060</td>\n",
       "      <td>0.026021</td>\n",
       "      <td>-0.743504</td>\n",
       "      <td>0.231869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125028</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027443</td>\n",
       "      <td>0.216407</td>\n",
       "      <td>-0.004552</td>\n",
       "      <td>-0.029960</td>\n",
       "      <td>-0.124695</td>\n",
       "      <td>-0.193445</td>\n",
       "      <td>0.147995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.090867</td>\n",
       "      <td>0.340160</td>\n",
       "      <td>0.156188</td>\n",
       "      <td>0.238270</td>\n",
       "      <td>0.080521</td>\n",
       "      <td>-0.287405</td>\n",
       "      <td>-0.055261</td>\n",
       "      <td>-0.035710</td>\n",
       "      <td>-0.153943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125029</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.320115</td>\n",
       "      <td>0.476275</td>\n",
       "      <td>1.615010</td>\n",
       "      <td>0.415784</td>\n",
       "      <td>-0.544668</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>-0.329363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263331</td>\n",
       "      <td>-0.382986</td>\n",
       "      <td>-0.618283</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>0.647461</td>\n",
       "      <td>-0.791091</td>\n",
       "      <td>-0.856748</td>\n",
       "      <td>-0.160233</td>\n",
       "      <td>-0.261460</td>\n",
       "      <td>0.811108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125030</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.069183</td>\n",
       "      <td>-0.078615</td>\n",
       "      <td>0.370626</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>-0.321495</td>\n",
       "      <td>-0.171112</td>\n",
       "      <td>0.108393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.372531</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.447317</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>-0.063160</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>0.070154</td>\n",
       "      <td>-0.125300</td>\n",
       "      <td>0.183153</td>\n",
       "      <td>0.403415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125031</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.248765</td>\n",
       "      <td>-0.207725</td>\n",
       "      <td>1.390730</td>\n",
       "      <td>-0.320359</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>-1.108583</td>\n",
       "      <td>0.899525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080677</td>\n",
       "      <td>0.850511</td>\n",
       "      <td>0.102414</td>\n",
       "      <td>-0.725563</td>\n",
       "      <td>0.409096</td>\n",
       "      <td>-0.305666</td>\n",
       "      <td>0.455545</td>\n",
       "      <td>2.263104</td>\n",
       "      <td>-1.306650</td>\n",
       "      <td>-1.413442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125032</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>-0.094303</td>\n",
       "      <td>0.394483</td>\n",
       "      <td>-0.125165</td>\n",
       "      <td>-0.208392</td>\n",
       "      <td>-0.205522</td>\n",
       "      <td>-0.040941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081695</td>\n",
       "      <td>0.151382</td>\n",
       "      <td>0.200634</td>\n",
       "      <td>0.118040</td>\n",
       "      <td>-0.096501</td>\n",
       "      <td>-0.046599</td>\n",
       "      <td>-0.281080</td>\n",
       "      <td>-0.270943</td>\n",
       "      <td>-0.104117</td>\n",
       "      <td>0.030278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125033</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.591740</td>\n",
       "      <td>-2.207826</td>\n",
       "      <td>1.364086</td>\n",
       "      <td>-1.996579</td>\n",
       "      <td>0.670188</td>\n",
       "      <td>-0.293931</td>\n",
       "      <td>-0.487370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787091</td>\n",
       "      <td>0.654590</td>\n",
       "      <td>0.930249</td>\n",
       "      <td>1.033835</td>\n",
       "      <td>0.161052</td>\n",
       "      <td>-0.080337</td>\n",
       "      <td>0.566934</td>\n",
       "      <td>-0.437990</td>\n",
       "      <td>-0.903721</td>\n",
       "      <td>-0.542193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125034</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.761341</td>\n",
       "      <td>0.240578</td>\n",
       "      <td>1.903857</td>\n",
       "      <td>-1.099916</td>\n",
       "      <td>-1.840773</td>\n",
       "      <td>0.610098</td>\n",
       "      <td>-0.600426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.434091</td>\n",
       "      <td>-0.477528</td>\n",
       "      <td>-1.059813</td>\n",
       "      <td>-0.615605</td>\n",
       "      <td>-0.642612</td>\n",
       "      <td>0.463703</td>\n",
       "      <td>-0.486226</td>\n",
       "      <td>1.373200</td>\n",
       "      <td>-0.674854</td>\n",
       "      <td>-1.764410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125035</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.792783</td>\n",
       "      <td>-1.542652</td>\n",
       "      <td>0.139961</td>\n",
       "      <td>1.305065</td>\n",
       "      <td>0.133135</td>\n",
       "      <td>0.138942</td>\n",
       "      <td>0.376361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174295</td>\n",
       "      <td>0.686349</td>\n",
       "      <td>0.520910</td>\n",
       "      <td>0.519352</td>\n",
       "      <td>0.723870</td>\n",
       "      <td>-1.212857</td>\n",
       "      <td>0.222622</td>\n",
       "      <td>-0.930402</td>\n",
       "      <td>-0.160861</td>\n",
       "      <td>-0.536785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125036</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.265484</td>\n",
       "      <td>0.068052</td>\n",
       "      <td>0.390755</td>\n",
       "      <td>-0.271873</td>\n",
       "      <td>-0.264071</td>\n",
       "      <td>-0.066970</td>\n",
       "      <td>0.061756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056689</td>\n",
       "      <td>0.231145</td>\n",
       "      <td>0.119842</td>\n",
       "      <td>0.097733</td>\n",
       "      <td>-0.274378</td>\n",
       "      <td>0.295230</td>\n",
       "      <td>-0.047191</td>\n",
       "      <td>-0.394332</td>\n",
       "      <td>0.087536</td>\n",
       "      <td>0.009543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125037</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.205158</td>\n",
       "      <td>-0.716052</td>\n",
       "      <td>-0.714486</td>\n",
       "      <td>-0.136872</td>\n",
       "      <td>-0.227463</td>\n",
       "      <td>-0.503945</td>\n",
       "      <td>0.337891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207343</td>\n",
       "      <td>0.364698</td>\n",
       "      <td>0.228496</td>\n",
       "      <td>-0.245175</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>-0.047791</td>\n",
       "      <td>0.540815</td>\n",
       "      <td>-0.287848</td>\n",
       "      <td>0.054226</td>\n",
       "      <td>0.114872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125038</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.113256</td>\n",
       "      <td>0.231709</td>\n",
       "      <td>0.791329</td>\n",
       "      <td>-0.244124</td>\n",
       "      <td>-0.301244</td>\n",
       "      <td>-0.227052</td>\n",
       "      <td>-0.287594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032961</td>\n",
       "      <td>0.289434</td>\n",
       "      <td>-0.412900</td>\n",
       "      <td>0.075387</td>\n",
       "      <td>-0.232368</td>\n",
       "      <td>0.272902</td>\n",
       "      <td>0.252090</td>\n",
       "      <td>-0.054866</td>\n",
       "      <td>-0.008398</td>\n",
       "      <td>-0.051513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125039</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.691276</td>\n",
       "      <td>0.300999</td>\n",
       "      <td>0.238516</td>\n",
       "      <td>0.076385</td>\n",
       "      <td>-0.417997</td>\n",
       "      <td>-0.129793</td>\n",
       "      <td>1.084011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381803</td>\n",
       "      <td>0.225830</td>\n",
       "      <td>-0.012399</td>\n",
       "      <td>0.180965</td>\n",
       "      <td>-0.423596</td>\n",
       "      <td>0.156815</td>\n",
       "      <td>-0.417152</td>\n",
       "      <td>0.096528</td>\n",
       "      <td>0.346723</td>\n",
       "      <td>-0.142967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125040</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.309337</td>\n",
       "      <td>-0.203942</td>\n",
       "      <td>-0.329721</td>\n",
       "      <td>-0.192191</td>\n",
       "      <td>-0.543000</td>\n",
       "      <td>-0.336039</td>\n",
       "      <td>-0.369258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263138</td>\n",
       "      <td>0.687956</td>\n",
       "      <td>0.092499</td>\n",
       "      <td>-0.314781</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>-0.005888</td>\n",
       "      <td>-0.141381</td>\n",
       "      <td>-0.146554</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.070513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125041</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>0.095532</td>\n",
       "      <td>-0.032851</td>\n",
       "      <td>-0.224068</td>\n",
       "      <td>-0.121686</td>\n",
       "      <td>0.094288</td>\n",
       "      <td>-0.912729</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136952</td>\n",
       "      <td>0.042265</td>\n",
       "      <td>-0.307760</td>\n",
       "      <td>-0.308834</td>\n",
       "      <td>0.244207</td>\n",
       "      <td>-0.483451</td>\n",
       "      <td>-0.072156</td>\n",
       "      <td>-0.731534</td>\n",
       "      <td>0.161044</td>\n",
       "      <td>-0.017657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125042</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.559303</td>\n",
       "      <td>-0.266228</td>\n",
       "      <td>0.129869</td>\n",
       "      <td>-0.210893</td>\n",
       "      <td>-0.338550</td>\n",
       "      <td>0.067228</td>\n",
       "      <td>-0.119278</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780795</td>\n",
       "      <td>0.241362</td>\n",
       "      <td>-0.376699</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>0.192810</td>\n",
       "      <td>0.554701</td>\n",
       "      <td>-0.015255</td>\n",
       "      <td>0.425633</td>\n",
       "      <td>-0.381032</td>\n",
       "      <td>0.419483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125043</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.182987</td>\n",
       "      <td>-0.359191</td>\n",
       "      <td>0.678681</td>\n",
       "      <td>-0.336450</td>\n",
       "      <td>-0.633629</td>\n",
       "      <td>-0.234663</td>\n",
       "      <td>0.124536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.330328</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.069921</td>\n",
       "      <td>0.050957</td>\n",
       "      <td>-0.382427</td>\n",
       "      <td>-0.833101</td>\n",
       "      <td>0.158809</td>\n",
       "      <td>0.093461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125044</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.195270</td>\n",
       "      <td>-0.797116</td>\n",
       "      <td>0.506561</td>\n",
       "      <td>-0.826037</td>\n",
       "      <td>-0.358254</td>\n",
       "      <td>-0.071085</td>\n",
       "      <td>0.021816</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370899</td>\n",
       "      <td>0.085515</td>\n",
       "      <td>0.130833</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.183732</td>\n",
       "      <td>0.771376</td>\n",
       "      <td>-0.424301</td>\n",
       "      <td>0.316099</td>\n",
       "      <td>0.391053</td>\n",
       "      <td>0.326632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125045</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.401045</td>\n",
       "      <td>-0.038219</td>\n",
       "      <td>0.674623</td>\n",
       "      <td>1.049426</td>\n",
       "      <td>-0.114915</td>\n",
       "      <td>-0.005331</td>\n",
       "      <td>0.361628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401488</td>\n",
       "      <td>-0.154087</td>\n",
       "      <td>0.088190</td>\n",
       "      <td>0.793239</td>\n",
       "      <td>-0.453124</td>\n",
       "      <td>0.293859</td>\n",
       "      <td>0.216596</td>\n",
       "      <td>-0.453377</td>\n",
       "      <td>-0.156412</td>\n",
       "      <td>0.190265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125046</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.824125</td>\n",
       "      <td>-0.995164</td>\n",
       "      <td>1.232163</td>\n",
       "      <td>0.628188</td>\n",
       "      <td>-0.457503</td>\n",
       "      <td>0.224323</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081346</td>\n",
       "      <td>0.057749</td>\n",
       "      <td>0.278974</td>\n",
       "      <td>0.910933</td>\n",
       "      <td>0.449319</td>\n",
       "      <td>0.202635</td>\n",
       "      <td>-0.509781</td>\n",
       "      <td>0.336324</td>\n",
       "      <td>-0.358628</td>\n",
       "      <td>0.367418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125047</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.500836</td>\n",
       "      <td>-0.464816</td>\n",
       "      <td>0.325525</td>\n",
       "      <td>-0.086527</td>\n",
       "      <td>-0.250825</td>\n",
       "      <td>0.126340</td>\n",
       "      <td>-0.979125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449596</td>\n",
       "      <td>-0.139438</td>\n",
       "      <td>0.114355</td>\n",
       "      <td>-0.139773</td>\n",
       "      <td>-0.246717</td>\n",
       "      <td>-0.239340</td>\n",
       "      <td>-1.632349</td>\n",
       "      <td>-0.872281</td>\n",
       "      <td>-0.108287</td>\n",
       "      <td>-0.023170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125048</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.441015</td>\n",
       "      <td>0.046359</td>\n",
       "      <td>0.514047</td>\n",
       "      <td>-0.245953</td>\n",
       "      <td>-0.303465</td>\n",
       "      <td>-0.407428</td>\n",
       "      <td>0.495768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225396</td>\n",
       "      <td>0.296466</td>\n",
       "      <td>0.147920</td>\n",
       "      <td>-0.282439</td>\n",
       "      <td>-0.362642</td>\n",
       "      <td>0.229445</td>\n",
       "      <td>0.172512</td>\n",
       "      <td>-0.370058</td>\n",
       "      <td>-0.001268</td>\n",
       "      <td>-0.179877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125049</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.052707</td>\n",
       "      <td>-0.226587</td>\n",
       "      <td>0.233317</td>\n",
       "      <td>-0.305317</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>-0.073208</td>\n",
       "      <td>0.195698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114958</td>\n",
       "      <td>0.270020</td>\n",
       "      <td>0.523976</td>\n",
       "      <td>-0.111749</td>\n",
       "      <td>-0.114235</td>\n",
       "      <td>-0.158290</td>\n",
       "      <td>-0.078994</td>\n",
       "      <td>-0.759816</td>\n",
       "      <td>0.358039</td>\n",
       "      <td>0.075459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125050</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.042817</td>\n",
       "      <td>0.953699</td>\n",
       "      <td>0.539441</td>\n",
       "      <td>-0.089796</td>\n",
       "      <td>-0.624817</td>\n",
       "      <td>0.320750</td>\n",
       "      <td>-0.097744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070292</td>\n",
       "      <td>-0.112886</td>\n",
       "      <td>0.485268</td>\n",
       "      <td>0.680431</td>\n",
       "      <td>0.323487</td>\n",
       "      <td>0.441991</td>\n",
       "      <td>-0.631725</td>\n",
       "      <td>-1.200069</td>\n",
       "      <td>-0.329841</td>\n",
       "      <td>0.345895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125051</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.513435</td>\n",
       "      <td>0.069538</td>\n",
       "      <td>0.373106</td>\n",
       "      <td>0.493198</td>\n",
       "      <td>0.198556</td>\n",
       "      <td>-0.049229</td>\n",
       "      <td>0.252034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510842</td>\n",
       "      <td>-0.510525</td>\n",
       "      <td>-1.751060</td>\n",
       "      <td>0.450280</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>-0.185300</td>\n",
       "      <td>-0.681235</td>\n",
       "      <td>-0.694613</td>\n",
       "      <td>-0.360631</td>\n",
       "      <td>0.502770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.816964</td>\n",
       "      <td>-0.524310</td>\n",
       "      <td>-0.350160</td>\n",
       "      <td>-0.295191</td>\n",
       "      <td>-0.871910</td>\n",
       "      <td>0.036080</td>\n",
       "      <td>-0.070158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254780</td>\n",
       "      <td>0.203225</td>\n",
       "      <td>1.091162</td>\n",
       "      <td>1.285139</td>\n",
       "      <td>-0.199406</td>\n",
       "      <td>0.480582</td>\n",
       "      <td>-0.082952</td>\n",
       "      <td>0.034913</td>\n",
       "      <td>-0.624448</td>\n",
       "      <td>-0.648182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125053</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.430116</td>\n",
       "      <td>-0.382682</td>\n",
       "      <td>0.626782</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>-0.204783</td>\n",
       "      <td>-0.762739</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034237</td>\n",
       "      <td>0.199284</td>\n",
       "      <td>-0.583744</td>\n",
       "      <td>0.059926</td>\n",
       "      <td>-0.108906</td>\n",
       "      <td>0.473123</td>\n",
       "      <td>-0.693718</td>\n",
       "      <td>0.193173</td>\n",
       "      <td>-0.212747</td>\n",
       "      <td>-0.273444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125054</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.360556</td>\n",
       "      <td>0.163471</td>\n",
       "      <td>0.792812</td>\n",
       "      <td>-0.245469</td>\n",
       "      <td>-0.056996</td>\n",
       "      <td>-0.390026</td>\n",
       "      <td>-0.153948</td>\n",
       "      <td>...</td>\n",
       "      <td>1.836177</td>\n",
       "      <td>1.093881</td>\n",
       "      <td>0.682960</td>\n",
       "      <td>0.629037</td>\n",
       "      <td>-1.129111</td>\n",
       "      <td>-0.482691</td>\n",
       "      <td>-0.181259</td>\n",
       "      <td>0.611031</td>\n",
       "      <td>0.705535</td>\n",
       "      <td>-0.199392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125055</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1.754420</td>\n",
       "      <td>0.731043</td>\n",
       "      <td>1.457615</td>\n",
       "      <td>1.774136</td>\n",
       "      <td>-0.363626</td>\n",
       "      <td>-1.200695</td>\n",
       "      <td>-1.076478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425286</td>\n",
       "      <td>1.634024</td>\n",
       "      <td>0.151937</td>\n",
       "      <td>1.207350</td>\n",
       "      <td>-1.101959</td>\n",
       "      <td>-0.225161</td>\n",
       "      <td>0.911666</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.688074</td>\n",
       "      <td>0.371423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2125056 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         clus_10  clus_15  clus_20         0         1         2         3  \\\n",
       "0              2       14       16  0.454749  0.075933  0.989870  0.763248   \n",
       "1              8        8       14  0.211989  0.485131 -0.756601  0.339424   \n",
       "2              1       13       19  0.517644 -0.483260  0.215278  1.264967   \n",
       "3              9        5       17 -0.731719 -0.488490 -0.591593  1.106247   \n",
       "4              5        5        1  0.160218 -0.414370  1.155223 -0.110876   \n",
       "5              5       13       11  0.104035  0.856233  1.013289 -0.253802   \n",
       "6              5        6       17 -0.931717  0.466346  0.820053 -0.319068   \n",
       "7              8       11        3  0.894262  0.480287  1.392853 -0.823165   \n",
       "8              6       11       11  0.484781 -0.156893 -0.570019 -0.098150   \n",
       "9              1        6        4  2.074618  0.186892  0.523038 -1.221658   \n",
       "10             3        3       14  0.319077  0.189111  0.735804  0.775532   \n",
       "11             9        8       14 -1.238495  0.343419  0.288339 -1.686531   \n",
       "12             5       11        2  1.532298 -0.068005 -0.431739  0.518090   \n",
       "13             1        8       14  1.266990 -0.453994  0.568036 -0.423422   \n",
       "14             9       13        4  1.789864 -0.113671 -0.638692 -0.583688   \n",
       "15             8       11        7  0.438000  0.643194  0.559813  0.556135   \n",
       "16             8       14        5  1.586111 -0.162036 -0.381543  0.152505   \n",
       "17             8       11       11  0.593465 -0.164775 -1.129564 -1.765515   \n",
       "18             7       12       17 -0.988596  0.175324 -0.777983  0.420834   \n",
       "19             5       14        7  0.838402  1.848023  1.901385 -0.825645   \n",
       "20             2       10        1 -0.718814  0.276568 -0.317922  0.180582   \n",
       "21             5        5        1 -0.998192 -0.652060 -0.422096  0.006808   \n",
       "22             2        1        2 -1.763630  2.071971  0.389227  0.320396   \n",
       "23             7        6        2 -1.358129 -0.345329  0.344043  1.658000   \n",
       "24             2       14        9 -0.854921 -1.086504 -1.961570 -0.992653   \n",
       "25             6        5        3 -0.564789 -0.459840  0.375916 -1.250074   \n",
       "26             2        2       18  1.206237 -1.563462  1.363556 -1.911910   \n",
       "27             8       14        3  0.301745 -0.926932  0.378548 -0.000342   \n",
       "28             9        6       19 -0.030745 -0.505959  0.018151 -0.250200   \n",
       "29             5        5        1 -0.085988  0.371518 -0.122883  0.895685   \n",
       "...          ...      ...      ...       ...       ...       ...       ...   \n",
       "2125026        2       10        5  0.775374 -0.861869  0.719364 -0.749225   \n",
       "2125027        0        3        4  0.678328 -0.676052  0.161326 -0.193364   \n",
       "2125028        2       10        5  0.027443  0.216407 -0.004552 -0.029960   \n",
       "2125029        3        6        9 -1.320115  0.476275  1.615010  0.415784   \n",
       "2125030        2       10        5  0.069183 -0.078615  0.370626  0.000861   \n",
       "2125031        3       10       13  0.248765 -0.207725  1.390730 -0.320359   \n",
       "2125032        2       10        5  0.060844 -0.094303  0.394483 -0.125165   \n",
       "2125033        9        6        2  1.591740 -2.207826  1.364086 -1.996579   \n",
       "2125034        0        4       12  1.761341  0.240578  1.903857 -1.099916   \n",
       "2125035        6        5        3  0.792783 -1.542652  0.139961  1.305065   \n",
       "2125036        2       10        5  0.265484  0.068052  0.390755 -0.271873   \n",
       "2125037        6        5        7  0.205158 -0.716052 -0.714486 -0.136872   \n",
       "2125038        2        1       17 -0.113256  0.231709  0.791329 -0.244124   \n",
       "2125039        1        9        6  0.691276  0.300999  0.238516  0.076385   \n",
       "2125040        0       10        5  0.309337 -0.203942 -0.329721 -0.192191   \n",
       "2125041        3       10        8  0.198373  0.095532 -0.032851 -0.224068   \n",
       "2125042        9       13       19  0.559303 -0.266228  0.129869 -0.210893   \n",
       "2125043        2        2        9  0.182987 -0.359191  0.678681 -0.336450   \n",
       "2125044        6        5        3 -0.195270 -0.797116  0.506561 -0.826037   \n",
       "2125045        8        1       11 -0.401045 -0.038219  0.674623  1.049426   \n",
       "2125046        7       12       15  0.824125 -0.995164  1.232163  0.628188   \n",
       "2125047        2        6       17 -0.500836 -0.464816  0.325525 -0.086527   \n",
       "2125048        2       10        5  0.441015  0.046359  0.514047 -0.245953   \n",
       "2125049        2       10        7  0.052707 -0.226587  0.233317 -0.305317   \n",
       "2125050        1        9        7 -0.042817  0.953699  0.539441 -0.089796   \n",
       "2125051        3        2        8  0.513435  0.069538  0.373106  0.493198   \n",
       "2125052        0        0       13  0.816964 -0.524310 -0.350160 -0.295191   \n",
       "2125053        3        6        8 -0.430116 -0.382682  0.626782  0.008205   \n",
       "2125054        3        4       16 -0.360556  0.163471  0.792812 -0.245469   \n",
       "2125055        0        2        9  1.754420  0.731043  1.457615  1.774136   \n",
       "\n",
       "                4         5         6    ...          140       141       142  \\\n",
       "0        0.790644 -0.251485 -0.316166    ...     0.451340 -0.205073  0.959361   \n",
       "1       -0.324144 -2.769078 -1.416410    ...     0.541693 -0.467021 -0.659951   \n",
       "2       -0.322865 -1.913158  0.256509    ...    -0.583523 -1.085528  1.698945   \n",
       "3       -1.671839  0.311964 -0.216956    ...     1.159797  0.346400 -2.014061   \n",
       "4       -0.330072  0.289665  0.792875    ...     0.194380 -0.244511  0.204626   \n",
       "5        1.496202  1.689694 -0.490203    ...     0.021130 -0.551046  0.404666   \n",
       "6       -0.958943 -1.393632  0.993937    ...    -0.923889 -0.264479 -1.290142   \n",
       "7        0.986922  0.650972  1.674797    ...     0.688301 -0.993085  0.248002   \n",
       "8       -0.078652 -0.264801  1.004178    ...    -0.402525 -1.039898 -0.820029   \n",
       "9       -0.575767 -0.601467  1.831380    ...     0.937187 -0.742368 -0.776615   \n",
       "10       0.546122 -1.863112 -2.048345    ...    -0.018419 -1.438737  0.933700   \n",
       "11       0.186190  0.620981  0.112574    ...    -0.997840  2.253281 -0.479863   \n",
       "12      -0.961575 -0.092375 -1.000568    ...     0.591483 -0.204968  1.518610   \n",
       "13      -0.300479  0.562464  0.980908    ...    -0.479555  0.624813 -0.465263   \n",
       "14      -2.120926 -0.727127 -0.526959    ...     0.805959  0.026632 -1.322826   \n",
       "15      -0.707759 -0.267755  1.000388    ...     0.236951 -0.076833 -1.385507   \n",
       "16      -0.004210  0.948998 -0.384120    ...     0.441705 -0.092109 -0.439628   \n",
       "17       1.155397 -0.862518 -0.198087    ...     0.277721 -0.549283  0.752677   \n",
       "18      -1.294988 -0.210168  0.124326    ...     0.617670 -0.033027  1.052647   \n",
       "19      -0.087709 -0.051559  2.279064    ...    -0.243672 -0.828519 -0.281909   \n",
       "20      -0.028598 -0.031075 -0.011744    ...     0.896637  0.012360 -1.071132   \n",
       "21      -0.890774 -0.302304 -0.356649    ...    -0.974528 -0.384752 -1.045449   \n",
       "22      -0.058987  0.661099 -1.855587    ...     0.901545 -0.019229  0.623758   \n",
       "23      -0.856290  1.158707 -0.548496    ...     0.071590 -0.822102 -1.099156   \n",
       "24      -2.385791 -0.412670 -0.312254    ...    -0.094751 -0.835020 -1.108176   \n",
       "25      -1.675026 -0.239494  1.446652    ...    -0.158307  1.896269 -0.576279   \n",
       "26      -0.214320  1.409319  0.272559    ...     1.266391  0.463811  1.795530   \n",
       "27       0.011444 -0.235807 -0.235270    ...    -0.620800 -0.082128  0.150910   \n",
       "28      -0.594774  0.295514 -0.216311    ...     2.223554 -0.438636  0.318239   \n",
       "29       1.162060 -0.294194  0.755688    ...     0.599642 -0.694994 -1.475784   \n",
       "...           ...       ...       ...    ...          ...       ...       ...   \n",
       "2125026 -0.924174 -0.019475  0.698560    ...    -0.214010 -0.141264  0.706353   \n",
       "2125027 -0.160909 -0.070157  0.120412    ...     0.272701  0.232765 -0.365083   \n",
       "2125028 -0.124695 -0.193445  0.147995    ...     0.020100  0.090867  0.340160   \n",
       "2125029 -0.544668  0.683010 -0.329363    ...     0.263331 -0.382986 -0.618283   \n",
       "2125030 -0.321495 -0.171112  0.108393    ...    -0.372531  0.050926  0.447317   \n",
       "2125031  0.004318 -1.108583  0.899525    ...     0.080677  0.850511  0.102414   \n",
       "2125032 -0.208392 -0.205522 -0.040941    ...    -0.081695  0.151382  0.200634   \n",
       "2125033  0.670188 -0.293931 -0.487370    ...    -0.787091  0.654590  0.930249   \n",
       "2125034 -1.840773  0.610098 -0.600426    ...    -0.434091 -0.477528 -1.059813   \n",
       "2125035  0.133135  0.138942  0.376361    ...    -0.174295  0.686349  0.520910   \n",
       "2125036 -0.264071 -0.066970  0.061756    ...    -0.056689  0.231145  0.119842   \n",
       "2125037 -0.227463 -0.503945  0.337891    ...    -0.207343  0.364698  0.228496   \n",
       "2125038 -0.301244 -0.227052 -0.287594    ...    -0.032961  0.289434 -0.412900   \n",
       "2125039 -0.417997 -0.129793  1.084011    ...     0.381803  0.225830 -0.012399   \n",
       "2125040 -0.543000 -0.336039 -0.369258    ...     0.263138  0.687956  0.092499   \n",
       "2125041 -0.121686  0.094288 -0.912729    ...    -1.136952  0.042265 -0.307760   \n",
       "2125042 -0.338550  0.067228 -0.119278    ...    -0.780795  0.241362 -0.376699   \n",
       "2125043 -0.633629 -0.234663  0.124536    ...     0.006272  0.047847  0.330328   \n",
       "2125044 -0.358254 -0.071085  0.021816    ...    -0.370899  0.085515  0.130833   \n",
       "2125045 -0.114915 -0.005331  0.361628    ...     0.401488 -0.154087  0.088190   \n",
       "2125046 -0.457503  0.224323  0.259769    ...    -0.081346  0.057749  0.278974   \n",
       "2125047 -0.250825  0.126340 -0.979125    ...    -0.449596 -0.139438  0.114355   \n",
       "2125048 -0.303465 -0.407428  0.495768    ...    -0.225396  0.296466  0.147920   \n",
       "2125049  0.045977 -0.073208  0.195698    ...     0.114958  0.270020  0.523976   \n",
       "2125050 -0.624817  0.320750 -0.097744    ...    -0.070292 -0.112886  0.485268   \n",
       "2125051  0.198556 -0.049229  0.252034    ...     0.510842 -0.510525 -1.751060   \n",
       "2125052 -0.871910  0.036080 -0.070158    ...    -0.254780  0.203225  1.091162   \n",
       "2125053 -0.204783 -0.762739  0.637291    ...    -0.034237  0.199284 -0.583744   \n",
       "2125054 -0.056996 -0.390026 -0.153948    ...     1.836177  1.093881  0.682960   \n",
       "2125055 -0.363626 -1.200695 -1.076478    ...     0.425286  1.634024  0.151937   \n",
       "\n",
       "              143       144       145       146       147       148       149  \n",
       "0        0.243185 -0.960646 -1.106796 -1.120893 -0.754034  0.301842 -0.711453  \n",
       "1        0.601387  0.631507  0.560771 -0.648289 -1.247034  0.201610 -0.238821  \n",
       "2       -0.438385  0.282447  0.990588 -0.360752  0.734434  0.957895 -0.641732  \n",
       "3       -0.528511 -1.246807  1.182403 -0.085697  1.188419 -2.269073  0.627365  \n",
       "4       -0.049117  0.685791 -0.149767  0.060965 -0.474822 -0.149615 -0.258662  \n",
       "5        0.234500 -0.248470 -1.250434  0.536455 -0.117993  1.255967 -0.708034  \n",
       "6       -1.570770 -0.458085  0.702444  0.403883  0.453409 -0.228465 -1.947498  \n",
       "7       -0.135104  0.337365 -0.442643 -1.246762  0.495055 -1.457670 -0.742216  \n",
       "8        0.548100 -1.238249 -1.091168  0.555323 -0.711731  0.009352 -0.502651  \n",
       "9       -0.923591  1.090211 -0.154619 -1.139419 -1.251905 -1.746036  0.599412  \n",
       "10       0.731022 -0.826978 -0.855350 -0.390728  1.940610 -0.387916  1.127038  \n",
       "11       1.162844 -0.851110  0.431434 -0.747527 -1.794214  0.811555  2.096765  \n",
       "12       0.444101 -0.342793  0.981514 -0.538393 -0.404810 -1.295646  0.065177  \n",
       "13      -0.383440  0.527489 -0.842416 -0.352087 -1.227127 -0.822321 -0.820280  \n",
       "14      -0.678000 -0.242413 -0.500054 -0.454481  0.047415 -1.799811  0.402607  \n",
       "15      -0.820939  0.123627  0.066654 -0.444164  0.701125 -0.623872  0.497334  \n",
       "16      -1.525549 -0.210573  1.337417  1.030276 -0.638091 -0.539717 -0.194077  \n",
       "17      -1.643069 -2.113327 -0.315380  0.453948 -1.285405  2.025407 -0.179060  \n",
       "18      -0.903474 -1.639873  1.109817 -1.363804 -0.396597  0.004654 -0.478790  \n",
       "19       0.950659  0.194299  0.793625  0.323166  0.768904 -0.419927 -0.104330  \n",
       "20       0.982362  0.424636  1.162848 -0.668527 -0.451883 -0.462184 -0.084663  \n",
       "21       0.307190  1.149969 -0.411039  0.214585 -1.041682 -0.756979  0.309792  \n",
       "22       0.348588  0.175954  0.376039  0.353263  1.895003 -0.472101  1.287589  \n",
       "23       0.239352  0.347223  2.392717  0.124059 -0.760795 -0.385483 -0.475890  \n",
       "24      -2.781423 -0.597115  1.106567  1.931757  0.022389 -0.160121  0.677379  \n",
       "25      -0.034878  0.963296 -1.188844  0.907049 -0.221215 -0.303974  1.178788  \n",
       "26       0.658047  0.583725  1.352628 -0.738504 -0.855608  0.271130 -0.046765  \n",
       "27       0.110486 -0.153438  0.006931  0.050974 -0.023183 -0.233290  0.360279  \n",
       "28       1.879167 -2.518092  1.056992  0.053190 -2.259646  0.312910  0.011311  \n",
       "29      -0.066147 -0.289666 -0.500087 -0.862967 -0.728902  0.789504  0.308620  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "2125026 -0.168059 -0.131127  0.141395  0.089671 -0.784405 -0.604153 -0.610378  \n",
       "2125027  0.328134 -0.281861  0.316981 -0.205060  0.026021 -0.743504  0.231869  \n",
       "2125028  0.156188  0.238270  0.080521 -0.287405 -0.055261 -0.035710 -0.153943  \n",
       "2125029  0.636075  0.647461 -0.791091 -0.856748 -0.160233 -0.261460  0.811108  \n",
       "2125030  0.082877 -0.063160  0.329840  0.070154 -0.125300  0.183153  0.403415  \n",
       "2125031 -0.725563  0.409096 -0.305666  0.455545  2.263104 -1.306650 -1.413442  \n",
       "2125032  0.118040 -0.096501 -0.046599 -0.281080 -0.270943 -0.104117  0.030278  \n",
       "2125033  1.033835  0.161052 -0.080337  0.566934 -0.437990 -0.903721 -0.542193  \n",
       "2125034 -0.615605 -0.642612  0.463703 -0.486226  1.373200 -0.674854 -1.764410  \n",
       "2125035  0.519352  0.723870 -1.212857  0.222622 -0.930402 -0.160861 -0.536785  \n",
       "2125036  0.097733 -0.274378  0.295230 -0.047191 -0.394332  0.087536  0.009543  \n",
       "2125037 -0.245175  0.143852 -0.047791  0.540815 -0.287848  0.054226  0.114872  \n",
       "2125038  0.075387 -0.232368  0.272902  0.252090 -0.054866 -0.008398 -0.051513  \n",
       "2125039  0.180965 -0.423596  0.156815 -0.417152  0.096528  0.346723 -0.142967  \n",
       "2125040 -0.314781  0.129397 -0.005888 -0.141381 -0.146554  0.104698  0.070513  \n",
       "2125041 -0.308834  0.244207 -0.483451 -0.072156 -0.731534  0.161044 -0.017657  \n",
       "2125042 -0.355832  0.192810  0.554701 -0.015255  0.425633 -0.381032  0.419483  \n",
       "2125043  0.011244  0.069921  0.050957 -0.382427 -0.833101  0.158809  0.093461  \n",
       "2125044  0.017487  0.183732  0.771376 -0.424301  0.316099  0.391053  0.326632  \n",
       "2125045  0.793239 -0.453124  0.293859  0.216596 -0.453377 -0.156412  0.190265  \n",
       "2125046  0.910933  0.449319  0.202635 -0.509781  0.336324 -0.358628  0.367418  \n",
       "2125047 -0.139773 -0.246717 -0.239340 -1.632349 -0.872281 -0.108287 -0.023170  \n",
       "2125048 -0.282439 -0.362642  0.229445  0.172512 -0.370058 -0.001268 -0.179877  \n",
       "2125049 -0.111749 -0.114235 -0.158290 -0.078994 -0.759816  0.358039  0.075459  \n",
       "2125050  0.680431  0.323487  0.441991 -0.631725 -1.200069 -0.329841  0.345895  \n",
       "2125051  0.450280  0.063968 -0.185300 -0.681235 -0.694613 -0.360631  0.502770  \n",
       "2125052  1.285139 -0.199406  0.480582 -0.082952  0.034913 -0.624448 -0.648182  \n",
       "2125053  0.059926 -0.108906  0.473123 -0.693718  0.193173 -0.212747 -0.273444  \n",
       "2125054  0.629037 -1.129111 -0.482691 -0.181259  0.611031  0.705535 -0.199392  \n",
       "2125055  1.207350 -1.101959 -0.225161  0.911666  0.130092  0.688074  0.371423  \n",
       "\n",
       "[2125056 rows x 153 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_dic = {'复仇者联盟2':'2015-05-12', '大鱼海棠':\"2016-07-08\", '美国队长3':\"2016-05-06\", '十二生肖':\"2012-12-20\",\n",
    "            '九层妖塔':\"2015-09-30\", '大圣归来':\"2015-07-10\", '栀子花开':\"2015-07-10\", '夏洛特烦恼':\"2015-09-30\",\n",
    "            '钢铁侠1':\"2008-04-30\", '西游降魔篇':\"2013-02-10\", '西游伏妖篇':\"2017-01-28\", '爱乐之城':\"2017-02-14\", \n",
    "            '泰囧':\"2012-12-12\", '何以笙箫默':\"2015-04-30\", '湄公河行动':\"2016-09-30\", '七月与安生':\"2016-09-14\",\n",
    "            '复仇者联盟':\"2012-05-05\", '后会无期':\"2014-07-24\", '寻龙诀':\"2015-12-18\", '长城':\"2016-12-16\", \n",
    "            '左耳':\"2015-04-24\", '美人鱼':\"2016-02-08\", '小时代1':\"2013-06-27\", '小时代3':\"2014-07-17\", \n",
    "            '釜山行':\"2016-07-20\",'变形金刚4':\"2014-06-27\", '你的名字':\"2016-12-02\", '疯狂动物城':\"2016-03-04\"}\n",
    "df['time_interval'] = df.apply(lambda x: (pd.to_datetime(x['Date']) - pd.to_datetime(name_dic[x['Movie_Name_CN']])).days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.594 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "stopwords = [line.strip() for line in open(\"stopword.txt\", 'r',encoding='utf-8').readlines()] #load stopwords\n",
    "\n",
    "def func(row):\n",
    "    tmp_list = jieba.cut(row['Comment'], cut_all = False)\n",
    "    mark_count1 = 0 #! ~\n",
    "    mark_count2 = 0 #?\n",
    "    mark_count3 = 0 #...\n",
    "    word_count = 0\n",
    "    outstr = \"\"\n",
    "    for word in tmp_list:\n",
    "        if (word == '!' or word == '~' or word == '！' or word ==\"√\" or word == \"★\" or word ==\"～\"):\n",
    "            mark_count1 += 1\n",
    "        if (word == '?'or word == '？'):\n",
    "            mark_count2 += 1\n",
    "        if (word == '...' or word == '......' or word == '。。。'):\n",
    "            mark_count3 += 1\n",
    "        if word not in stopwords:\n",
    "            word_count += 1\n",
    "            outstr += word + \" \"\n",
    "    return pd.Series([outstr, mark_count1, mark_count2, mark_count3, word_count])\n",
    "\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp = df.apply(func, axis=1)\n",
    "df['new_comment'] = tmp[0]\n",
    "df['mark!'] = tmp[1]\n",
    "df['mark?'] = tmp[2]\n",
    "df['mark...'] = tmp[3]\n",
    "df['word_num'] = tmp[4]\n",
    "\n",
    "\n",
    "name_dummy = pd.get_dummies(df['Movie_Name_CN'])\n",
    "df = pd.concat([df,name_dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_count = df.groupby(['Movie_Name_CN'])['ID'].agg(['count']).reset_index()\n",
    "tmp_count.columns = ['Movie_Name_CN', 'movie_count']\n",
    "df = pd.merge(df, tmp_count, how='left', on='Movie_Name_CN')\n",
    "\n",
    "tmp_count = df.groupby(['Movie_Name_CN', 'Date'])['ID'].agg(['count']).reset_index()\n",
    "tmp_count.columns = ['Movie_Name_CN', 'Date', 'comment_day_num']\n",
    "df = pd.merge(df, tmp_count, how='left', on=['Movie_Name_CN', 'Date'])\n",
    "\n",
    "df['comment_day_prop'] = df['comment_day_num'] / df['movie_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Movie_Name_EN', 'Movie_Name_CN', 'Crawl_Date', 'Number',\n",
       "       'Username', 'Date', 'Star', 'Comment', 'Like', 'time_interval', 'mark!',\n",
       "       'mark?', 'mark...', 'word_num', 'new_comment', '七月与安生', '九层妖塔', '何以笙箫默',\n",
       "       '你的名字', '十二生肖', '变形金刚4', '后会无期', '复仇者联盟', '复仇者联盟2', '夏洛特烦恼', '大圣归来',\n",
       "       '大鱼海棠', '寻龙诀', '小时代1', '小时代3', '左耳', '栀子花开', '泰囧', '湄公河行动', '爱乐之城',\n",
       "       '疯狂动物城', '美人鱼', '美国队长3', '西游伏妖篇', '西游降魔篇', '釜山行', '钢铁侠1', '长城',\n",
       "       'movie_count', 'comment_day_num', 'comment_day_prop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['ID', 'Movie_Name_EN', 'Movie_Name_CN', 'Crawl_Date', 'Number','Username', 'Date',  \n",
    "         'Comment', 'new_comment', 'movie_count'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df,res],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([         'Star',          'Like', 'time_interval',         'mark!',\n",
       "               'mark?',       'mark...',      'word_num',         '七月与安生',\n",
       "                '九层妖塔',         '何以笙箫默',\n",
       "       ...\n",
       "                   140,             141,             142,             143,\n",
       "                   144,             145,             146,             147,\n",
       "                   148,             149],\n",
       "      dtype='object', length=190)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Star'] = df['Star'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature = list(df.columns)\n",
    "feature.remove('Star')\n",
    "\n",
    "train = df.sample(frac=0.8, random_state=12345678)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(train[feature],train['Star'],test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4448709012355483, total=  47.6s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.44411340842916386, total=  46.8s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4433719206606903, total=  49.4s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.44549082813000906, total=  49.2s\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.44486501907291237, total= 1.0min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.44411340842916386, total= 1.0min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.44352779934589776, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.44548494589817267, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.44485325474764054, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4441104673391959, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.44405425754688127, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.44549082813000906, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.44485031366632255, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.44410458515926005, total= 1.0min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4440513164396132, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.44549376924592726, total= 1.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.45447941390131497, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4547660362930502, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.45320110115056117, total= 1.5min\n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4561464911016538, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4547660362930502, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.45432647767278117, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4561376677538992, total= 1.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4530834568598386, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.45454251345548663, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4541647182002935, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4531775722924167, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.45591708406003406, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 18.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.45416177711897554, total= 1.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.45453369018558276, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4531540434342721, total= 1.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4556376780478049, total= 1.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.45980571216813576, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4586482750507338, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.458592150772923, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.46022287776428134, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.45928808185617526, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4587659186494515, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.46011993870714424, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.45831568668972494, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4598351229813153, total= 1.7min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4591306138054763, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.4577274652361121, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.46044346145814646, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4600262932669825, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.45869827358018883, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.45876861720900686, total= 1.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=40, num_leaves=15, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.46045816703773745, total= 1.9min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.46693783436418235, total= 4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.46653333725478663, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.46596550669396014, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.46656392368392413, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4668290143554179, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.46677450663215786, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4654243429566363, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4671815580267465, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.46688195381914116, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.46680097644186935, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.465456695136585, total= 4.4min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75, score=0.46725214480878335, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4665290240609863, total= 4.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4669303844004588, total= 4.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.46606550434107435, total= 4.2min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=5, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1.4, subsample=0.75, score=0.4672139103018467, total= 4.2min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.4720906088332436, total= 5.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.47152730802035236, total= 5.6min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.47064480835745043, total=10.9min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1, subsample=0.75, score=0.47198145920525164, total=10.8min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4709671157697839, total= 7.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.47167436251874945, total= 7.5min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4718138155979142, total= 5.1min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n",
      "[CV]  boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1, reg_lambda=1.4, subsample=0.75, score=0.4700242347238888, total= 5.2min\n",
      "[CV] boosting_type=gbdt, colsample_bytree=0.66, learning_rate=0.5, n_estimators=200, num_leaves=10, objective=binary, random_state=501, reg_alpha=1.2, reg_lambda=1, subsample=0.75 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-888257d85410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                     n_jobs=2)\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Run the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'multiclass',\n",
    "          'num_class': 5,\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5 }\n",
    "\n",
    "# Create parameters to search\n",
    "gridParams = {\n",
    "    'learning_rate': [0.5, 0.005],\n",
    "    'n_estimators': [40, 200, 1000],\n",
    "    'num_leaves': [5, 10, 15],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.66],\n",
    "    'subsample' : [0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.4],\n",
    "    }\n",
    "\n",
    "# Create classifier to use\n",
    "clf = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "          max_depth = params['max_depth'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'])\n",
    "\n",
    "grid = GridSearchCV(clf, gridParams,\n",
    "                    verbose=3,\n",
    "                    cv=4,\n",
    "                    n_jobs=2)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-bdbdd5af2d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m gbm.fit(X_train, y_train,\n\u001b[1;32m      4\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "gbm = lgb.LGBMClassifier(**grid.best_params_)\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "y_pred=gbm.predict(test[feature])\n",
    "y_pred=[list(x).index(max(x)) for x in y_pred]\n",
    "print(y_pred)\n",
    "print(accuracy_score(test['Star'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              column  importance\n",
      "3              mark?           0\n",
      "4            mark...           0\n",
      "83                44           0\n",
      "53                14           0\n",
      "122               83           0\n",
      "126               87           0\n",
      "179              140           1\n",
      "99                60           1\n",
      "103               64           1\n",
      "155              116           1\n",
      "78                39           1\n",
      "95                56           1\n",
      "88                49           2\n",
      "89                50           2\n",
      "151              112           2\n",
      "167              128           2\n",
      "60                21           2\n",
      "57                18           2\n",
      "125               86           3\n",
      "124               85           3\n",
      "150              111           3\n",
      "163              124           3\n",
      "58                19           3\n",
      "96                57           3\n",
      "144              105           3\n",
      "182              143           3\n",
      "113               74           4\n",
      "97                58           4\n",
      "104               65           4\n",
      "170              131           4\n",
      "..               ...         ...\n",
      "157              118          75\n",
      "98                59          75\n",
      "70                31          76\n",
      "21                左耳          79\n",
      "11             变形金刚4          80\n",
      "13             复仇者联盟          82\n",
      "64                25          86\n",
      "17              大鱼海棠          89\n",
      "31               釜山行          92\n",
      "33                长城          92\n",
      "148              109          92\n",
      "22              栀子花开          93\n",
      "67                28          95\n",
      "8              何以笙箫默          96\n",
      "68                29          97\n",
      "20              小时代3         100\n",
      "116               77         103\n",
      "7               九层妖塔         103\n",
      "29             西游伏妖篇         104\n",
      "16              大圣归来         110\n",
      "94                55         114\n",
      "107               68         114\n",
      "19              小时代1         133\n",
      "26             疯狂动物城         136\n",
      "129               90         196\n",
      "186              147         217\n",
      "34   comment_day_num         227\n",
      "5           word_num         339\n",
      "0               Like         378\n",
      "1      time_interval         569\n",
      "\n",
      "[189 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame({\n",
    "        'column': feature,\n",
    "        'importance': clf.feature_importance(),\n",
    "    }).sort_values(by='importance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "validation_data=lgb.Dataset(X_test,label=y_test)\n",
    "params={\n",
    "    'learning_rate':0.1,\n",
    "    'lambda_l1':0.2,\n",
    "    'lambda_l2':0.2,\n",
    "    'max_depth':4,\n",
    "    'objective':'multiclass',\n",
    "    'num_class':5,  #lightgbm.basic.LightGBMError: b‘Number of classes should be specified and greater than 1 for multiclass training‘\n",
    "}\n",
    "clf=lgb.train(params,train_data,valid_sets=[validation_data])\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "y_pred=clf.predict(test[feature])\n",
    "y_pred=[list(x).index(max(x)) for x in y_pred]\n",
    "print(y_pred)\n",
    "print(accuracy_score(test['Star'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.2721455010871859e-06"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df['comment_day_prop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031.6022944550668"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff['count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Movie_Name_EN', 'Movie_Name_CN', 'Crawl_Date', 'Number',\n",
       "       'Username', 'Date', 'Star', 'Comment', 'Like', 'time_interval', 'mark!',\n",
       "       'mark?', 'mark...', 'word_num', 'new_comment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
